{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "num_epochs_target = 10\n",
    "num_epochs_shadow = 10\n",
    "num_epochs_attack = 10\n",
    "\n",
    "# NOTE: this code is only tested on CPU, there may be some issues on GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "# define transform and load dataset\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "\n",
    "train_set = dsets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
    "train_set = TensorDataset(torch.tensor(train_set.data), torch.tensor(train_set.targets))\n",
    "test_set = dsets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "device: cpu\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "split_size = len(train_set) // 4\n",
    "\n",
    "data_train_shadow = Subset(train_set, range(0, split_size))\n",
    "data_out_shadow = Subset(train_set, range(split_size, split_size * 2))\n",
    "data_train_attack = Subset(train_set, range(0, split_size * 2))\n",
    "data_train_target = Subset(train_set, range(split_size * 2, split_size * 3))\n",
    "data_nonmember_target = Subset(train_set, range(split_size * 3, len(train_set)))\n",
    "data_eval_attack = Subset(train_set, range(split_size * 2, len(train_set)))\n",
    "\n",
    "\n",
    "# make sure the splitted dataset are transformed\n",
    "class TransformedTensorDataset(TensorDataset):\n",
    "    def __init__(self, data_tensor, target_tensor, transform=None):\n",
    "        assert data_tensor.size(0) == target_tensor.size(0)\n",
    "        self.data_tensor = data_tensor\n",
    "        self.target_tensor = target_tensor\n",
    "        self.transform = transform\n",
    "        self.tensors = (data_tensor, target_tensor)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.data_tensor[index]\n",
    "        target = self.target_tensor[index]\n",
    "        data = Image.fromarray(data.numpy())\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "        return data, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_tensor.size(0)\n",
    "\n",
    "\n",
    "def subset_to_tensor(subset):\n",
    "    return TransformedTensorDataset(\n",
    "        torch.stack([subset[i][0] for i in range(len(subset))]),\n",
    "        torch.tensor([subset[i][1] for i in range(len(subset))]),\n",
    "        transform=transform,\n",
    "    )\n",
    "\n",
    "\n",
    "data_train_shadow = subset_to_tensor(data_train_shadow)\n",
    "data_out_shadow = subset_to_tensor(data_out_shadow)\n",
    "data_train_attack = subset_to_tensor(data_train_attack)\n",
    "data_train_target = subset_to_tensor(data_train_target)\n",
    "data_nonmember_target = subset_to_tensor(data_nonmember_target)\n",
    "data_eval_attack = subset_to_tensor(data_eval_attack)\n",
    "\n",
    "# Create labels for training of attack model\n",
    "label_train_attack = torch.cat(\n",
    "    (torch.ones(len(data_train_shadow)), torch.zeros(len(data_out_shadow))), dim=0\n",
    ")\n",
    "data_train_attack = TransformedTensorDataset(\n",
    "    data_train_attack.tensors[0], label_train_attack, transform=transform\n",
    ")\n",
    "\n",
    "# create labels for evaluating the attack model\n",
    "label_eval_attack = torch.cat(\n",
    "    (torch.ones(len(data_train_target)), torch.zeros(len(data_nonmember_target))), dim=0\n",
    ")\n",
    "data_eval_attack = TransformedTensorDataset(\n",
    "    data_eval_attack.tensors[0], label_eval_attack, transform=transform\n",
    ")\n",
    "\n",
    "\n",
    "loader_train_shadow = DataLoader(data_train_shadow, batch_size=64, shuffle=True)\n",
    "loader_out_shadow = DataLoader(data_out_shadow, batch_size=64, shuffle=True)\n",
    "loader_train_attack = DataLoader(data_train_attack, batch_size=64, shuffle=True)\n",
    "loader_train_target = DataLoader(data_train_target, batch_size=64, shuffle=True)\n",
    "loader_nonmember_target = DataLoader(data_nonmember_target, batch_size=64, shuffle=True)\n",
    "loader_eval_attack = DataLoader(data_eval_attack, batch_size=64, shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size=3):\n",
    "        super(CNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=input_size, out_channels=48, kernel_size=(3, 3)\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(in_channels=48, out_channels=96, kernel_size=(3, 3))\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        if input_size == 3:\n",
    "            self.fc_features = 6 * 6 * 96\n",
    "        else:\n",
    "            self.fc_features = 5 * 5 * 96\n",
    "        self.fc1 = nn.Linear(in_features=self.fc_features, out_features=512)\n",
    "        self.fc2 = nn.Linear(in_features=512, out_features=128)\n",
    "        self.fc3 = nn.Linear(in_features=128, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, self.fc_features)  # reshape x\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "target_model = CNN().to(device)\n",
    "shadow_model = CNN().to(device)\n",
    "target_loss = nn.CrossEntropyLoss()\n",
    "shadow_loss = nn.CrossEntropyLoss()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "optimizer_target = torch.optim.Adam(target_model.parameters(), lr=0.001)\n",
    "loss = 999\n",
    "for epoch in range(num_epochs_target):\n",
    "    print(f\"epoch {epoch}/{num_epochs_target} loss={loss}\")\n",
    "    loss = 0\n",
    "    for i, (images, labels) in tqdm(enumerate(loader_train_target)):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs_target = target_model(images)\n",
    "        loss_target = target_loss(outputs_target, labels)\n",
    "        optimizer_target.zero_grad()\n",
    "        loss_target.backward()\n",
    "        optimizer_target.step()\n",
    "        loss += loss_target\n",
    "target_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in loader_train_target:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        target = target_model(images)\n",
    "        _, prediction = torch.max(target, dim=1)\n",
    "        correct += (prediction == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    print(f\"target model accu (train set): {correct}/{total}={correct/total*100:.2f}%\")\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in loader_nonmember_target:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        target = target_model(images)\n",
    "        _, prediction = torch.max(target, dim=1)\n",
    "        correct += (prediction == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    print(f\"target model accu (test set): {correct}/{total}={correct/total*100:.2f}%\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0/10 loss=999\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "196it [00:11, 17.03it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 1/10 loss=338.8419189453125\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "196it [00:11, 17.17it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 2/10 loss=265.96044921875\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "196it [00:11, 17.02it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 3/10 loss=228.33941650390625\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "196it [00:11, 17.05it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 4/10 loss=198.8133544921875\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "196it [00:11, 16.82it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 5/10 loss=164.3293914794922\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "196it [00:11, 16.97it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 6/10 loss=133.46311950683594\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "196it [00:11, 16.95it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 7/10 loss=102.7719955444336\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "196it [00:11, 17.00it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 8/10 loss=70.64253234863281\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "196it [00:11, 16.99it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 9/10 loss=45.383392333984375\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "196it [00:11, 16.99it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "target model accu (train set): 12086/12500=96.69%\n",
      "target model accu (test set): 7738/12500=61.90%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "optimizer_shadow = torch.optim.Adam(shadow_model.parameters(), lr=0.001)\n",
    "loss = 999\n",
    "for epoch in range(num_epochs_shadow):\n",
    "    print(f\"epoch {epoch}/{num_epochs_target} loss={loss}\")\n",
    "    loss = 0\n",
    "    for i, (images, labels) in tqdm(enumerate(loader_train_shadow)):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs_shadow = shadow_model(images)\n",
    "        loss_shadow = shadow_loss(outputs_shadow, labels)\n",
    "        optimizer_shadow.zero_grad()\n",
    "        loss_shadow.backward()\n",
    "        optimizer_shadow.step()\n",
    "        loss += loss_shadow\n",
    "shadow_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in loader_nonmember_target:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        target = shadow_model(images)\n",
    "        _, prediction = torch.max(target, dim=1)\n",
    "        correct += (prediction == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    print(f\"shadow model accu: {correct}/{total}={correct/total*100:.2f}%\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0/10 loss=999\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "196it [00:11, 16.67it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 1/10 loss=335.2061462402344\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "196it [00:11, 17.25it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 2/10 loss=266.6365966796875\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "196it [00:11, 17.04it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 3/10 loss=226.57061767578125\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "196it [00:11, 17.02it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 4/10 loss=194.8171844482422\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "196it [00:11, 17.12it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 5/10 loss=163.1771240234375\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "196it [00:11, 16.98it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 6/10 loss=132.09356689453125\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "196it [00:11, 17.26it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 7/10 loss=98.14667510986328\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "196it [00:11, 17.31it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 8/10 loss=66.05852508544922\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "196it [00:11, 17.03it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 9/10 loss=41.381263732910156\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "196it [00:11, 17.17it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "shadow model accu: 7715/12500=61.72%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class AttackModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AttackModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "attack_model = AttackModel().to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "optimizer_attack = torch.optim.Adam(attack_model.parameters(), lr=0.001)\n",
    "loss = 999\n",
    "for epoch in range(num_epochs_attack):\n",
    "    print(f\"epoch {epoch}/{num_epochs_attack}, loss={loss}\")\n",
    "    loss = 0\n",
    "    for idx, (images, labels) in tqdm(enumerate(loader_train_attack)):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        posteriors_shadow = F.softmax(shadow_model(images), dim=1)\n",
    "        top3_posteriors = torch.topk(posteriors_shadow, 3, dim=1)[0]\n",
    "        labels_attack = labels.float().unsqueeze(1)\n",
    "\n",
    "        # attack_outputs = attack_model(posteriors_shadow)\n",
    "        attack_outputs = attack_model(top3_posteriors)\n",
    "        loss_attack = F.binary_cross_entropy(attack_outputs, labels_attack)\n",
    "\n",
    "        optimizer_attack.zero_grad()\n",
    "        loss_attack.backward()\n",
    "        optimizer_attack.step()\n",
    "        loss += loss_attack"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0/10, loss=999\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "391it [00:21, 18.35it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 1/10, loss=262.6524963378906\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "391it [00:22, 17.29it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 2/10, loss=254.68312072753906\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "391it [00:24, 16.17it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 3/10, loss=254.04678344726562\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "391it [00:24, 16.09it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 4/10, loss=253.76869201660156\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "391it [00:22, 17.12it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 5/10, loss=253.57717895507812\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "391it [00:25, 15.11it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 6/10, loss=253.39329528808594\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "391it [00:35, 10.91it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 7/10, loss=253.30189514160156\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "391it [00:34, 11.39it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 8/10, loss=253.22560119628906\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "391it [00:34, 11.34it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 9/10, loss=253.23565673828125\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "391it [00:34, 11.43it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "attack_model.eval()\n",
    "total = 0\n",
    "\n",
    "all_pred = torch.empty((1))\n",
    "all_labels = torch.empty((1))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in loader_eval_attack:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        posteriors_target = F.softmax(target_model(images), dim=1)\n",
    "        top3_posteriors = torch.topk(posteriors_target, 3, dim=1)[0]\n",
    "\n",
    "        # attack_outputs = attack_model(posteriors_target)\n",
    "        attack_outputs = attack_model(top3_posteriors)\n",
    "\n",
    "        all_pred = torch.cat((all_pred, attack_outputs.squeeze()), dim=0)\n",
    "        all_labels = torch.cat((all_labels, labels.squeeze()), dim=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "for pred_thre in np.arange(0.20, 0.65, 0.05):\n",
    "    all_pred_bin = all_pred > pred_thre\n",
    "    all_labels_bin = all_labels > 0.5  # only 0 or 1\n",
    "\n",
    "    TP = (all_pred_bin & all_labels_bin).sum().item()\n",
    "    TN = ((~all_pred_bin) & (~all_labels_bin)).sum().item()\n",
    "    FP = (all_pred_bin & (~all_labels_bin)).sum().item()\n",
    "    FN = ((~all_pred_bin) & all_labels_bin).sum().item()\n",
    "\n",
    "    print(\n",
    "        f\"Attack Model Accuracy for thre={pred_thre:.2f} TP={TP} TN={TN} FP={FP} FN={FN} \",\n",
    "        end=\"\",\n",
    "    )\n",
    "    if (TP + FP) == 0 or (TP + FN) == 0:\n",
    "        print(\"metrics invalid\")\n",
    "        continue\n",
    "    accu = (TP + TN) / (TP + TN + FP + FN)\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    print(f\"accu={accu:.2f} \" f\"prec={precision:.2f} recall={recall:.2f} f1={f1:.2f}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Attack Model Accuracy for thre=0.20 TP=12334 TN=927 FP=11574 FN=166 accu=0.53 prec=0.52 recall=0.99 f1=0.68\n",
      "Attack Model Accuracy for thre=0.25 TP=12132 TN=1690 FP=10811 FN=368 accu=0.55 prec=0.53 recall=0.97 f1=0.68\n",
      "Attack Model Accuracy for thre=0.30 TP=11770 TN=2564 FP=9937 FN=730 accu=0.57 prec=0.54 recall=0.94 f1=0.69\n",
      "Attack Model Accuracy for thre=0.35 TP=11389 TN=3468 FP=9033 FN=1111 accu=0.59 prec=0.56 recall=0.91 f1=0.69\n",
      "Attack Model Accuracy for thre=0.40 TP=10971 TN=4252 FP=8249 FN=1529 accu=0.61 prec=0.57 recall=0.88 f1=0.69\n",
      "Attack Model Accuracy for thre=0.45 TP=10642 TN=4806 FP=7695 FN=1858 accu=0.62 prec=0.58 recall=0.85 f1=0.69\n",
      "Attack Model Accuracy for thre=0.50 TP=10192 TN=5412 FP=7089 FN=2308 accu=0.62 prec=0.59 recall=0.82 f1=0.68\n",
      "Attack Model Accuracy for thre=0.55 TP=9543 TN=6121 FP=6380 FN=2957 accu=0.63 prec=0.60 recall=0.76 f1=0.67\n",
      "Attack Model Accuracy for thre=0.60 TP=8156 TN=7322 FP=5179 FN=4344 accu=0.62 prec=0.61 recall=0.65 f1=0.63\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 }
}